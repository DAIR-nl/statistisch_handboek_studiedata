---
title: "Multipele lineaire regressie"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: 
      collapsed: FALSE 
    number_sections: true
  keywords: [statistisch handboek, studiedata]
---

<!-- ## CLOSEDBLOK: Functies.R -->
```{r functies, include = TRUE, echo = FALSE, results='asis', warning=FALSE, message=FALSE}
library(here)
if (!exists("Substitute_var")) {
  ## Installeer packages en functies
  source(paste0(here::here(), "/99. Functies en Libraries/00. Voorbereidingen.R"), echo = FALSE)
}
```
<!-- ## /CLOSEDBLOK: Functies.R -->

<!-- ## CLOSEDBLOK: CSS -->
<style>
`r htmltools::includeHTML(paste0(here::here(),"/01. Includes/css/Stylesheet_SHHO.css"))`
</style>
<!-- ## /CLOSEDBLOK: CSS -->

<!-- ## CLOSEDBLOK: Header.R -->
```{r header, include = TRUE, echo = FALSE, results='asis', code =  readLines(paste0(here::here(),"/01. Includes/code/Header.R"))} 
```
<!-- ## /CLOSEDBLOK: Header.R -->

<!-- ## CLOSEDBLOK: Status.R -->
```{r status, include = TRUE, echo = FALSE, results='asis', code =  readLines(paste0(here::here(),"/01. Includes/code/Status.R"))} 
```
<!-- ## /CLOSEDBLOK: Status.R -->

<!-- ## CLOSEDBLOK: Reticulate.R -->

<!-- ## /CLOSEDBLOK: Reticulate.R -->

<!-- ## OPENBLOK: Data-aanmaken.R -->
```{r aanmaken data, include=FALSE, echo=TRUE}
source(paste0(here::here(),"/01. Includes/data/28.R"))
```
<!-- ## /OPENBLOK: Data-aanmaken.R -->

# Toepassing

Gebruik *multipele lineaire regressie* om te toetsen of een continue afhankelijke variabele voorspeeld kan worden met twee of meer onafhankelijke variabelen (predictors) en om te toetsen of er een relatie is tussen een predictor en de afhankelijke variabele in aanwezigheid van andere predictors.[^1]


# Onderwijscasus
<div id = "casus">

De docent van het van Methoden & Statistiek van de bachelor Onderwijskunde wil zijn studenten ervan overtuigen dat het nuttig is om naar zijn colleges te komen. Hij wil daarom onderzoeken of er een relatie is tussen het aantal colleges dat studenten volgen en het eindcijfer voor zijn vak. Daarnaast vermoedt hij dat het eindexamencijfer voor wiskunde ook gerelateerd is aan het eindcijfer voor het vak en dat er man-vrouw verschillen zouden kunnen zijn. Op basis van de data van vorig jaar onderzoekt hij of er een relatie is tussen het aantal hoorcolleges dat studenten volgen, waarbij hij rekening houdt met het eindexamencijfer wiskunde en man-vrouw verschillen. Met de resultaten hiervan hoopt hij zijn studenten ervan te overtuigen om de hoorcolleges van het vak bij te wonen.

Dit onderzoek vertaalt zich in de volgende combinatie van hypothesen, waarbij de nulhypothese zo geformuleerd is dat er geen effect of verschil is en de alternatieve hypothese zo geformuleerd is dat er wel een effect of verschil is. Bij *multipele lineaire regressie* wordt er een hypothese opgesteld voor het complete model en hypotheses voor de individuele predictors.

Hypotheses regressiemodel:

*H~0~*: Het toevoegen van de predictors geslacht, eindexamencijfer wiskunde en aantal gevolgde hoorcolleges zorgt niet voor een toename in de verklaarde variantie van het model ($\Delta R^2 = 0$).

*H~A~*: Het toevoegen van de predictors geslacht, eindexamencijfer wiskunde en aantal gevolgde hoorcolleges zorgt voor een toename in de verklaarde variantie van het model ($\Delta R^2 > 0$).

Hypotheses individuele predictors (met als voorbeeld de predictor aantal hoorcolleges):

*H~0~*: Het aantal hoorcolleges heeft geen voorspellende waarde voor het eindcijfer Methoden & Statistiek in aanwezigheid van de andere predictoren geslacht en eindexamencijfer wiskunde ($\beta = 0$).

*H~A~*: Het aantal hoorcolleges heeft voorspellende waarde voor het eindcijfer Methoden & Statistiek in aanwezigheid van de andere predictoren geslacht en eindexamencijfer wiskunde ($\beta \neq 0$).

</div>

# Multipele lineaire regressie

Het doel van multipele lineaire regressie is het voorspellen van een afhankelijke variabele op basis van meerdere onafhankelijke variabelen, ook wel predictors genoemd.[^1] Het voorspellen van een afhankelijke variabele wordt gedaan met behulp van een regressievergelijking waarin de afhankelijke variabele een lineaire combinatie is van de predictors, i.e. 

$$ y_i = \beta_0 + \beta_1 * x_{1i} + \beta_2 * x_{2i} + e_i$$

met $y_i$ als afhankelijke variabele, $\beta_0$ als intercept, $x_{1i}$ en $x_{2i}$ als predictors, $\beta_{1}$ en $\beta_{2}$ als regressiecoefficiënten van predictors, $e_i$ als residu of error en $i$ als indicator van de observatie uit de steekproef. Voor de huidige casus is de volgende regressievergelijking op te stellen:

$$ CijferStatistiek_i = \beta_0 + \beta_1 * Geslacht_i + \beta_2 * EindexamencijferWiskunde_i + \beta_3 * Aantal Hoorcolleges_i + e_i$$

De regressiecoefficiënten ($\beta_0$, $\beta_1$, $\beta_2$ en $\beta_3$) worden geschat door de waardes voor deze coefficiënten te vinden waarbij de som van de kwadraten van de residu $e_i$ zo klein mogelijk is. Multipele regressie wordt daarom ook wel ordinary least squares (OLS) regressie genoemd, omdat men de oplossing vind met de kleinste kwadraten van de residuën. De methode kan als volgt visueel weergegeven worden:

<div class = "col-container">
  <div class="col">
<!-- ## OPENBLOK: QQplot1.R -->
```{r, echo = FALSE, message = FALSE}
set.seed(12345)
x <- rnorm(100,5,2)
y <- 3 + 1.2 * x + rnorm(100,0,1)
dataset <- data.frame(Afhankelijke_variabele = y, Predictor = x)

model <- lm(Afhankelijke_variabele ~ 1, data = dataset)
dataset$Fit <- fitted(model)

ggplot(dataset, aes(x = Predictor, y = Afhankelijke_variabele)) +
    geom_point() + ylab("Afhankelijke variabele") + 
    geom_smooth(method = "lm", se = FALSE, formula = "y ~1") + 
    geom_segment(aes(x = Predictor, y = Afhankelijke_variabele,
                   xend = Predictor, yend = Fit)) +
  ggtitle("Interceptmodel")

```
<!-- ## /OPENBLOK: QQplot1.R -->
  </div>
  <div class = "col">
<!-- ## OPENBLOK: QQplot2.R -->
```{r, echo = FALSE, message = FALSE}
set.seed(12345)
x <- rnorm(100,5,2)
y <- 3 + 1.2 * x + rnorm(100,0,1)
dataset <- data.frame(Afhankelijke_variabele = y, Predictor = x)

model <- lm(Afhankelijke_variabele ~ Predictor, data = dataset)
dataset$Fit <- fitted(model)

ggplot(dataset, aes(x = Predictor, y = Afhankelijke_variabele)) +
    geom_point() + geom_smooth(method = "lm", se = FALSE) + ylab("Afhankelijke variabele") +
    geom_segment(aes(x = Predictor, y = Afhankelijke_variabele,
                   xend = Predictor, yend = Fit)) + 
  ggtitle("Regressiemodel")

```
<!-- ## /OPENBLOK: QQplot2.R -->
  </div>
</div>

Beide figuren laten een manier zien om een goede voorspelling te maken. De punten zijn de observaties, de blauwe lijn is de voorspelling en de zwarte lijnen zijn het verschil tussen observatie en voorspelling oftewel het residu. In de linkerfiguur wordt de voorspelling gemaakt met behulp van alleen een intercept (horizontale lijn), dit heet een interceptmodel. Echter,in de rechterfiguur wordt de voorspelling gemaakt door een lijn te trekken waarbij de kwadraten van de residuën zo klein mogelijk zijn. Het trekken van de best passende lijn is in feite wat er gedaan wordt bij multipele lineaire regressie.

## F-test voor significantie regressiemodel

Een maat om de kwaliteit van de voorspellingen van het regressiemodel te kwantificeren is de verklaarde variantie van de afhankelijke variabele. De verklaarde variantie - ook wel $R^2$ genoemd - wordt berekend als 1 minus de onverklaarde variantie. De onverklaarde variantie wordt berekend door de som van gekwadrateerde residuën van het regressiemodel te delen door de som van gekwadrateerde residuën van het interceptmodel. De kwaliteit van het regressiemodel wordt dus altijd vergeleken ten opzichte van het interceptmodel. Een goed regressiemodel heeft kleine residuën, wat leidt tot een lage onverklaarde variantie en dus een hoge verklaarde variantie.

De eerste vraag bij multipele lineaire regressie is of het model betere voorspellingen geeft dan een model zonder predictors. Daarom wordt het voorgestelde model vergeleken met een interceptmodel. Men verwacht een toename in verklaarde variantie wanneer beide modellen vergeleken worden. De significantie van deze toename wordt getoetst met een F-toets. De getoetste nulhypothese is dat de verklaarde variantie nul is ($R^2 = 0$) en de getoetste alternatieve hypothese is dat de verklaarde variantie groter dan nul is ($R^2 > 0$). Wanneer de toename in verklaarde variantie significant is, kunnen de individuele predictors getoetst worden. Wanneer de toename niet significant is, is de analyse afgelopen en is de conclusie dat het voorgestelde model niet in staat is om de afhankelijke variabele beter te voorspellen dan een model zonder predictors. De verklaarde variantie $R^2$ wordt ook gebruikt als effectmaat bij multipele lineaire regressie.[^2] Een verklaarde variantie rond 0,02 kan geïnterpreteerd worden als een klein effect, rond 0,13 als een gemiddeld effect en rond 0,26 als een groot effect.

De F-toets kan ook gebruikt worden om verschillende regressiemodellen onderling te vergelijken. Hierbij wordt getoetst of er een verschil is tussen de verklaarde variantie van beide modellen. Men kan bijvoorbeeld in eerste instantie een model met twee predictors willen toetsen en daarna willen toetsen of de voorspellingen van het regressiemodel nog beter worden met twee extra predictors. Het verschil in verklaarde variantie tussen het model met twee en vier predictors wordt dan getoetst met een F-toets. Hierbij is de nulhypothese dat het verschil in verklaarde variantie nul is ($\Delta R^2 = 0$) en de alternatieve hypothese dat het verschil in verklaarde variantie groter dan nul is ($\Delta R^2 > 0$). Een eis voor deze toets is dat beide modellen genest zijn. Bij geneste modellen is het ene model te schrijven als een versie van het andere model na het verwijderen van een aantal predictors. Het verschil tussen beide regressiemodellen moet dus toe te wijzen zijn aan het toevoegen of verwijderen van een of meerdere predictors.

## Individuele predictors toetsen en interpreteren

Als de F-toets aantoont dat het regressiemodel betere voorspellingen geeft dan het interceptmodel, kunnen de predictors getoetst worden. De regressiecoefficiënten van de predictors en de intercept worden getoetst met een t-toets waarbij de nulhypothese is dat de coefficiënt gelijk aan nul is ($\beta = 0$) en de (tweezijdige) alternatieve hypothese dat de coefficiënt niet gelijk aan nul is ($\beta \neq 0$). Er kan ook een eenzijdige alternatieve hypothese opgesteld worden, bijvoorbeeld wanneer men verwacht dat de coefficiënt positief of negatief is. De t-toets toont in feite aan of er wel of geen relatie is tussen de predictor en afhankelijke variabele in de aanwezigheid van de andere predictoren van het regressiemodel.

Op basis van de geschatte regressiecoefficiënten kan de regressievergelijking ingevuld worden. Op basis van de data zou bijvoorbeeld de volgende regressievergelijking gemaakt kunnen worden

$$ CijferStatistiek_i = 3 + 0,5 * Geslacht_i + 0,4 * EindexamencijferWiskunde_i + 0,3 * AantalHoorcolleges_i  +  e_i$$
waarbij alle regressiecoefficiënten significant zijn. Neem aan dat de variabele Geslacht de waarde 1 heeft voor vrouwen en 0 voor mannen. De regressiecoefficiënten geven informatie over de relatie van predictoren met de afhankelijke variabele:

* Intercept ($\beta_0$): De intercept geeft de waarde van de afhankelijke variabele weer wanneer alle predictors gelijk aan nul zijn. Wanneer een observatie dus de waarde 0 voor geslacht heeft (dus een man is) en een 0 voor eindexamencijfer wiskunde heeft, dan is het voorspelde cijfer statistiek een 3. De intercept geeft soms geen realistische waarde voor de afhankelijke variabele omdat het onwaarschijnlijk is dat de predictor gelijk is aan nul. Met een eindexamencijfer wiskunde dat gelijk is aan een nul zou iemand nooit bij de bachelor Onderwijskunde terecht kunnen komen. An sich is dit geen groot probleem, omdat het niet altijd nodig is de intercept te kunnen interpreteren. Een oplossing is het centeren van de predictors wat inhoudt dat per predictor het gemiddelde van die predictor van alle waarden wordt afgehaald. Op die manier ontstaat een predictor met een gemiddelde van nul. In die situatie is de intercept gelijk aan de voorspelde waarde van de afhankelijke variabele voor een observatie die voor alle predictors gelijk is aan het gemiddelde. Het is in feite de voorspelde waarde voor een gemiddelde observatie en daarom erg interessant.
* Geslacht ($\beta_1$): De coefficiënt van de variabele Geslacht is gelijk aan 0,5. Wanneer de overige predictors gelijkblijven is het cijfer statistiek 0,5 punten hoger voor vrouwen dan voor mannen. Immers, het product van predictor en coëfficient is $0,5 * 0 = 0$ voor mannen en $0,5 * 1 = 0,5$ voor vrouwen. Bij een categorische predictor geeft de coefficiënt informatie over het verschil tussen de twee categorieën. Let daarom goed op hoe de categorieën gecodeerd zijn, i.e. welke categorie de waarde 1 en 0 hebben.
* Eindexamencijfer wiskunde: De coëfficient van de variabele Eindexamencijfer Wiskunde is gelijk aan 0,4. Dit betekent dat een toename van 1 van de variabele Eindexamencijfer Wiskunde zorgt voor een toename van 0,4 op de afhankelijke variabele Cijfer statistiek wanneer de andere predictors gelijkblijven. Er is dus een positieve relatie tussen het eindexamencijfef wiskunde en het cijfer statistiek. Een observatie met een eindexamencijfer wiskunde dat één punt hoger is dan een andere observatie zal dus een voorspelde waarde voor het cijfer statistiek hebben van 0,4 punt hoger wanneer de overige predictors gelijkblijven.
* Aantal hoorcolleges: De coëfficient van de variabele Aantal hoorcolleges is gelijk aan 0,3. Dit betekent dat een toename van 1 van de variabele Aantal hoorcolleges zorgt voor een toename van 0,3 op de afhankelijke variabele Cijfer statistiek wanneer de andere predictors gelijkblijven. Er is dus een positieve relatie tussen het aantal gevolgde hoorcolleges en het cijfer statistiek. Een observatie met een extra gevolgd hoorcollege dan een andere observatie zal dus een voorspelde waarde voor het cijfer statistiek hebben van 0,3 punt hoger wanneer de overige predictors gelijkblijven.

## Predictors vergelijkingen door standaardisatie

Op basis van de regressievergelijking en de t-toetsen kan bepaald worden of predictors gerelateerd zijn aan de afhankelijke variabele en wat de invloed van een verandering van de predictor is op de afhankelijke variabele. Een andere relevante vraag is welke predictor het sterkst gerelateerd is aan de afhankelijke variabele. Op basis van de regressiecoëfficienten kan dit niet bepaald worden, omdat de schaal van de predictors verschilt. Het eindexamencijfer wiskunde varieert tussen de 1 en 10 terwijl de variabele Geslacht gelijk is aan een 1 of een 0. Standaardiseer de afhankelijke variabele en de predictors om de predictors onderling te kunnen vergelijken. Standaardiseren houdt in dat de variabelen getransformeerd worden zodat ze een gemiddelde van 0 hebben en een standaardafwijking van 1. Dit wordt gedaan door voor alle observaties van een variabele eerst het gemiddelde af te trekken en daarna te delen door de standaardafwijking, i.e. $\frac{x_i - \mu}{\sigma}$. Wanneer het regressiemodel opnieuw geschat wordt met gestandaardiseerde variabelen, kunnen de coëfficiënten onderling vergeleken worden op basis van hun grootte. Een grotere (absolute waarde van de) coëfficient betekent een sterkere relatie met de afhankelijke variabele. De coefficient is nu te interpreteren in termen van standaardafwijkingen. Een toename van één eenheid van de predictor staat voor een toename van een standaardafwijking van deze predictor. Deze toename van één standaardafwijking zorg ervoor dat de afhankelijke variabele $b$ standaardafwijkingen toeneemt waarbij $b$ de coefficient van de regressievergelijking met gestandaardiseerde variabelen is.

## Voorspellen op basis van het regressiemodel

Een regressiemodel maakt het mogelijk om een afhankelijke variabele te voorspellen op basis van een aantal predictors. Wanneer van een student bekend is wat zijn geslacht, eindexamencijfer wiskunde en aantal gevolgde hoorcolleges is, kan een voorspelling gemaakt worden van het cijfer statistiek voor die student. Op basis van deze voorspellingen wordt het regressiemodel geschat wat leidt tot de schattingen voor de coefficienten. Voor alle observaties in de steekproef is er dus een voorspelling. De regressievergelijking maakt het echter ook mogelijk om voor nieuwe observaties een voorspelling te maken. Dit wordt een out-of-sample voorspelling genoemd omdat de nieuwe observatie niet in de steekproef zit waarop het regressiemodel wordt geschat. Als dat de docent een jaar later de cijfers wilt voorspellen van de studenten die zijn vak volgen, kan hij de regressievergelijking van het jaar ervoor gebruiken. Een mannelijke student met een 7 als eindexamencijfer wiskunde die 8 hoorcolleges volgt, heeft een voorspeld cijfer voor het vak Methoden & Statistiek 

$$ CijferStatistiek_i = 3 + 0,5 * 0 + 0,4 * 7 + 0,3 * 8 = 8,2$$
een 8,2. Een out-of-sample voorspelling geeft vaak een goede indicatie van de kwaliteit van het regressiemodel, omdat de nieuwe observaties niet gebruikt zijn om het model te schatten. Als het doel is om het regressiemodel te gebruiken voor het voorspellen van nieuwe observaties, dan is de kwaliteit van de out-of-sample voorspellingen van belang.

# Assumpties

Outliers

Gewone outliers, mahalanobis leverage, cooks influence

# Linearity
# Homoskedasticity

# Normality
# Multicollinearity

Uitleggen bij schenden assumpties wat te doen voor elke assumptie apart

# Bootstrappen

# Uitvoering

## Outliers

## Assumpties

## Uitvoering

F-test

Coefficienten: significantie en interpreteren

Sterkste bijdrage berkenen

# Bootstrappen uitvoeren

# Rapportage





<!-- ## CLOSEDBLOK: Footer.R -->
```{r footer, include = TRUE, echo = FALSE, results='asis', code =  readLines(paste0(here::here(),"/01. Includes/code/Footer.R"))}
```
<!-- ## /CLOSEDBLOK: Footer.R -->

[^1]: Laerd Statistics (2018). *Multiple Regression Analysis using SPSS Statistics*. https://statistics.laerd.com/spss-tutorials/multiple-regression-using-spss-statistics.php
[^2]: Field, A. (2013). *Discovering statistics using IBM SPSS statistics*. Sage.





[^9]: Field, A., Miles, J., & Field, Z. (2012). *Discovering statistics using R*. London: Sage publications.
[^10]: In dit voorbeeld wordt uitgegaan van een waarschijnlijkheid van 95% c.q. een p-waardegrens van 0,05. De grens is naar eigen inzicht aan te passen; houd hierbij rekening met type I en type II fouten. 
[^11]: Stat 504. *6.2.3 - More on Goodness-of-Fit and Likelihood ratio tests*. [PennState Eberly College of Science](https://online.stat.psu.edu/stat504/node/220/).
[^12]: Stat 504. *8.1 - Polytomous (Multinomial) Logistic Regression*. [PennState Eberly College of Science](https://online.stat.psu.edu/stat504/node/172/).
[^13]: Stat 504. *8.2 - Baseline-Category Logit Model*. [PennState Eberly College of Science](https://online.stat.psu.edu/stat504/node/173/).



